{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert annotations to the format used by ATLOP for finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nltk version: 3.9.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\samue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "print('Nltk version: {}.'.format(nltk.__version__))\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "from nltk.tokenize import WordPunctTokenizer as wpt\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths to the annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PLATINUM_TRAIN = r\"C:\\Users\\samue\\OneDrive\\Desktop\\ThesisPiron\\train_platinum.json\"\n",
    "#PATH_GOLD_TRAIN = \"../Annotations/Train/gold_quality/json_format/train_gold.json\"\n",
    "#PATH_SILVER_TRAIN = \"../Annotations/Train/silver_quality/json_format/train_silver.json\"\n",
    "#PATH_BRONZE_TRAIN = \"../Annotations/Train/bronze_quality/json_format/train_bronze.json\"\n",
    "#PATH_DEV = \"../Annotations/Dev/json_format/dev.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUTPUT_PLATINUM_TRAIN = r\"C:\\Users\\samue\\OneDrive\\Desktop\\ThesisPiron\\train_platinumv2.json\"\n",
    "#PATH_OUTPUT_GOLD_TRAIN = \"../Train/RE/data/train_gold.json\"\n",
    "#PATH_OUTPUT_SILVER_TRAIN = \"../Train/RE/data/train_silver.json\"\n",
    "#PATH_OUTPUT_BRONZE_TRAIN = \"../Train/RE/data/train_bronze.json\"\n",
    "#PATH_OUTPUT_DEV = \"../Train/RE/data/dev.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input files into dictionary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_PLATINUM_TRAIN, 'r', encoding='utf-8') as file:\n",
    "\ttrain_platinum = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Remove articles having less that 5 entities annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_articles_with_less_than_5_entities(data: dict):\n",
    "    pmid_list = []\n",
    "    for pmid, article in data.items():\n",
    "        entities = article['entities']\n",
    "        if len(entities) < 5:\n",
    "            pmid_list.append(pmid)\n",
    "    return pmid_list\n",
    "\n",
    "def remove_articles_with_less_than_5_entities(data: dict, data_name: str):\n",
    "    pmid_list = get_articles_with_less_than_5_entities(data)\n",
    "    for pmid in pmid_list:\n",
    "        del data[pmid]\n",
    "    print(f'{data_name} - {len(pmid_list)} articles removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_platinum - 0 articles removed.\n"
     ]
    }
   ],
   "source": [
    "remove_articles_with_less_than_5_entities(train_platinum, \"train_platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles without relations annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_articles_without_relations(data: dict, data_name: str):\n",
    "    pmid_list = []\n",
    "    for pmid, article in data.items():\n",
    "        if len(article['relations']) == 0:\n",
    "            pmid_list.append(pmid)\n",
    "\n",
    "    for pmid in pmid_list:\n",
    "        del data[pmid]\n",
    "    print(f'{data_name} - {len(pmid_list)} articles removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_articles_without_relations(train_platinum, \"train_platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokens of length 2 that are not captured by the tokenizer\n",
    "ILLEGAL_WORDS_2 = [').', '(<', '>)', '),', '.,', '].', '],', '.:', '>.', '>,', '))', '+)', '>-', '</', '[<', '-,', '.)', 'â„¢,']\n",
    "# Define the tokens of length 3 that are not captured by the tokenizer\n",
    "ILLEGAL_WORDS_3 = ['.),', '.].', '>),', '.).', '>).']\n",
    "\n",
    "def tokenize_docs(data: dict, data_name: str):\n",
    "\tprint(f\"Tokenizing articles in set {data_name}...\")\n",
    "\n",
    "\tfor pmid, article in data.items():\n",
    "\t\ttitle = article['metadata']['title']\n",
    "\t\tabstract = article['metadata']['abstract']\n",
    "\n",
    "\t\ttitle_spans = list(wpt().span_tokenize(title))\n",
    "\t\tabstract_spans = list(wpt().span_tokenize(abstract))\n",
    "\t\t\n",
    "\t\tarticle['tokenized_title'] = []\n",
    "\t\tfor start, end in title_spans:\n",
    "\t\t\tword = title[start:end]\n",
    "\t\t\tif word in ILLEGAL_WORDS_2:\n",
    "\t\t\t\tword1 = title[start:end-1]\n",
    "\t\t\t\tword2 = title[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_title'].append((word1, start, end-1))\n",
    "\t\t\t\tarticle['tokenized_title'].append((word2, end-1, end))\n",
    "\t\t\telif word in ILLEGAL_WORDS_3:\n",
    "\t\t\t\tword1 = title[start:start+1]\n",
    "\t\t\t\tword2 = title[start+1:end-1]\n",
    "\t\t\t\tword3 = title[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_title'].append((word1, start, start+1))\n",
    "\t\t\t\tarticle['tokenized_title'].append((word2, start+1, end-1))\n",
    "\t\t\t\tarticle['tokenized_title'].append((word3, end-1, end))\n",
    "\t\t\telse:\t\n",
    "\t\t\t\tarticle['tokenized_title'].append((word, start, end))\n",
    "\t\t\n",
    "\t\tarticle['tokenized_abstract'] = []\n",
    "\t\tfor start, end in abstract_spans:\n",
    "\t\t\tword = abstract[start:end]\n",
    "\t\t\tif word in ILLEGAL_WORDS_2:\n",
    "\t\t\t\tword1 = abstract[start:end-1]\n",
    "\t\t\t\tword2 = abstract[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word1, start, end-1))\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word2, end-1, end))\n",
    "\t\t\telif word in ILLEGAL_WORDS_3:\n",
    "\t\t\t\tword1 = abstract[start:start+1]\n",
    "\t\t\t\tword2 = abstract[start+1:end-1]\n",
    "\t\t\t\tword3 = abstract[end-1:end]\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word1, start, start+1))\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word2, start+1, end-1))\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word3, end-1, end))\n",
    "\t\t\telse:\n",
    "\t\t\t\tarticle['tokenized_abstract'].append((word, start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing articles in set train_platinum...\n"
     ]
    }
   ],
   "source": [
    "tokenize_docs(train_platinum, \"train_platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Adjust wrong annotations (i.e., annotations including partially annotated words) from the silver collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Each PMIDs maps to a dict of {annotation_with_wrong_text_span: correct_text_span}\n",
    "PARTIAL_WORDS = {\n",
    "    '35275534': {\n",
    "        (1395, 1412, 'abstract', 'Ruminococcusgnavus'): 'Ruminococcusgnavusgroup'  \n",
    "    },\n",
    "    '38963982': {\n",
    "        (92, 94, 'title', 'TAM'): 'TAMs',\n",
    "        (435, 451, 'abstract', 'Intestinal tissue'): 'Intestinal tissues'\n",
    "    },\n",
    "    '38959280': {\n",
    "        (74, 76, 'abstract', 'SGM'): 'SGMs',\n",
    "        (695, 697, 'abstract', 'SGM'): 'SGMs',\n",
    "        (266, 287, 'abstract', 'cisgender heterosexual'): 'cisgender heterosexuals',\n",
    "        (713, 734, 'abstract', 'cisgender-heterosexual'): 'cisgender-heterosexuals' \n",
    "    },\n",
    "    '38968876': {\n",
    "        (764, 770, 'abstract', 'patient'): 'patients'\n",
    "    },\n",
    "    '38892525': {\n",
    "        (1397, 1407, 'abstract', 'IBS symptom'): 'IBS symptoms'\n",
    "    }\n",
    "}\n",
    "\n",
    "def fix_wrong_annotations(data: dict, data_name: str):\n",
    "    print(f'Fixing annotations in set {data_name}...')\n",
    "    \n",
    "    for pmid in list(data.keys()):\n",
    "        pmid_str = str(pmid)\n",
    "        if pmid_str in PARTIAL_WORDS:\n",
    "            replacements = PARTIAL_WORDS[pmid_str]\n",
    "            \n",
    "            # Fix entities:\n",
    "            for entity in data[pmid]['entities']:\n",
    "                wrong_entry = (entity['start_idx'], entity['end_idx'], entity['location'], entity['text_span'])\n",
    "                if wrong_entry in replacements:\n",
    "                    correct_text = replacements[wrong_entry]\n",
    "                    entity['text_span'] = correct_text\n",
    "                    # Update end index based on the new text length.\n",
    "                    entity['end_idx'] = entity['start_idx'] + len(correct_text) - 1\n",
    "                    print(f\"Fixed entity in pmid {pmid}: '{wrong_entry[3]}' -> '{correct_text}'\")\n",
    "            \n",
    "            # Fix relations:\n",
    "            for relation in data[pmid]['relations']:\n",
    "                # Check and fix subject if needed.\n",
    "                wrong_entry = (relation['subject_start_idx'], relation['subject_end_idx'], relation['subject_location'], relation['subject_text_span'])\n",
    "                if wrong_entry in replacements:\n",
    "                    correct_text = replacements[wrong_entry]\n",
    "                    relation['subject_text_span'] = correct_text\n",
    "                    relation['subject_end_idx'] = relation['subject_start_idx'] + len(correct_text) - 1\n",
    "                    print(f\"Fixed subject in pmid {pmid}: '{wrong_entry[3]}' -> '{correct_text}'\")\n",
    "                    \n",
    "                # Check and fix object if needed.\n",
    "                wrong_entry = (relation['object_start_idx'], relation['object_end_idx'], relation['object_location'], relation['object_text_span'])\n",
    "                if wrong_entry in replacements:\n",
    "                    correct_text = replacements[wrong_entry]\n",
    "                    relation['object_text_span'] = correct_text\n",
    "                    relation['object_end_idx'] = relation['object_start_idx'] + len(correct_text) - 1\n",
    "                    print(f\"Fixed object in pmid {pmid}: '{wrong_entry[3]}' -> '{correct_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#fix_wrong_annotations(train_silver, 'train_silver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map annotated entities to tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entities_to_tokens(data: dict, data_name: str):\n",
    "\tprint(f\"Mapping entities to tokens in set {data_name}...\")\n",
    "\t\n",
    "\tfor pmid, article in data.items():\n",
    "\t\tfor entity in article['entities']:\n",
    "\t\t\tlocation = entity['location']\n",
    "\t\t\tstart = entity['start_idx']\n",
    "\t\t\tend = entity['end_idx']\n",
    "\t\t\tstart_token = None\n",
    "\t\t\tend_token = None\n",
    "\t\t\tif location == 'title':\n",
    "\t\t\t\tfor idx, token in enumerate(article['tokenized_title']):\n",
    "\t\t\t\t\tif start == token[1] and start is not None:\n",
    "\t\t\t\t\t\tstart_token = idx\n",
    "\t\t\t\t\tif end == token[2]-1 and end is not None:\n",
    "\t\t\t\t\t\tend_token = idx\n",
    "\t\t\telif location == 'abstract':\n",
    "\t\t\t\tfor idx, token in enumerate(article['tokenized_abstract']):\n",
    "\t\t\t\t\tif start == token[1] and start is not None:\n",
    "\t\t\t\t\t\tstart_token = idx\n",
    "\t\t\t\t\tif end == token[2]-1 and end is not None:\n",
    "\t\t\t\t\t\tend_token = idx\n",
    "\t\t\telse:\n",
    "\t\t\t\traise Exception(f'{pmid} - Unrecognized Location: {location}')\n",
    "\t\t\tif start_token is not None and end_token is not None:\n",
    "\t\t\t\tentity['start_token'] = start_token\n",
    "\t\t\t\tentity['end_token'] = end_token\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint (data[pmid]['tokenized_title'])\n",
    "\t\t\t\tprint(data[pmid]['tokenized_abstract'])\n",
    "\t\t\t\traise Exception(f'{pmid} - Not able to assign token(s) to entity: {entity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping entities to tokens in set train_platinum...\n"
     ]
    }
   ],
   "source": [
    "map_entities_to_tokens(train_platinum, \"train_platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the start and end indices for articles sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define abbreviations to not be splitted by the sentence tokenizer\n",
    "extra_abbrevs = {'etc', 'etc.', 'etc.)', '<i>L', 'sp', 'subsp', '<i>A', '(<i>Hippophae rhamnoides</i> L.)'}\n",
    "punkt_param = PunktParameters()\n",
    "for abbr in extra_abbrevs:\n",
    "\tpunkt_param.abbrev_types.add(abbr)\n",
    "sentence_splitter = PunktSentenceTokenizer(punkt_param)\n",
    "\n",
    "def get_sentence_spans(data: dict, data_name: str):\n",
    "    print(f\"Getting sentence spans in set {data_name}...\")\n",
    "    \n",
    "    for pmid, article in data.items():\n",
    "        title = article['metadata']['title']\n",
    "        abstract = article['metadata']['abstract']\n",
    "\n",
    "        # Convert the generator to a list so we can iterate it repeatedly.\n",
    "        sentences = list(sentence_splitter.span_tokenize(abstract))\n",
    "\n",
    "        # Prepare a list of booleans that will flag whether the sentence at a given index should be merged with the next one. \n",
    "        # A sentence is merged with the following one if an entity spans across them.\n",
    "        # Initially, no merge is flagged.\n",
    "        merge_next = [False] * len(sentences)\n",
    "        \n",
    "        # Process each entity in the article.\n",
    "        for entity in article['entities']:\n",
    "            location = entity['location']\n",
    "            start = entity['start_idx']\n",
    "            end = entity['end_idx']\n",
    "\n",
    "            # For title entities, we do nothing regarding sentence spans.\n",
    "            if location == 'title':\n",
    "                if end > len(title):\n",
    "                    raise Exception(f'{pmid} - Found title entity having illegal end index: {entity}')\n",
    "                continue\n",
    "\n",
    "            # Only process abstract entities.\n",
    "            if location == 'abstract':\n",
    "                start_sentence = None\n",
    "                end_sentence = None\n",
    "\n",
    "                # Iterate over the original sentence spans to determine in which sentences the entity start and end fall.\n",
    "                for idx, s in enumerate(sentences):\n",
    "                    # Using >= and <= to include boundaries.\n",
    "                    if start >= s[0] and start <= s[1] and start_sentence is None:\n",
    "                        start_sentence = idx\n",
    "                        #print(f'Start sentence assigned: {idx}')\n",
    "                    if end >= s[0] and end <= s[1] and end_sentence is None:\n",
    "                        end_sentence = idx\n",
    "                        #print(f'End sentence assigned: {idx}')\n",
    "\n",
    "                if start_sentence is None:\n",
    "                    raise Exception(f'{pmid} - Start sentence not assigned for entity: {entity}')\n",
    "                if end_sentence is None:\n",
    "                    raise Exception(f'{pmid} - End sentence not assigned for entity: {entity}')\n",
    "                \n",
    "                # If the entity falls in two different sentences, check if they are consecutive.\n",
    "                if start_sentence != end_sentence:\n",
    "                    if end_sentence - start_sentence == 1:\n",
    "                        # Mark that sentence 'start_sentence' should be merged with its following sentence.\n",
    "                        merge_next[start_sentence] = True\n",
    "                        #print(f'{pmid} - Marking merge for sentences {start_sentence} and {end_sentence} due to entity: {entity}')\n",
    "                    else:\n",
    "                        raise Exception(f'{pmid} - Entity assigned to two non-consecutive sentences ({start_sentence}, {end_sentence}): {entity}')\n",
    "        \n",
    "        # At this point, we have a merge flag for each sentence that should be merged with its next one.\n",
    "        # Now we build the updated list of sentence spans, merging as flagged.\n",
    "        new_spans = []\n",
    "        i = 0\n",
    "        while i < len(sentences):\n",
    "            start_val = sentences[i][0]\n",
    "            end_val = sentences[i][1]\n",
    "            # While the current sentence is flagged to merge with the next one, update the end_val.\n",
    "            while i < len(sentences) - 1 and merge_next[i]:\n",
    "                i += 1\n",
    "                end_val = sentences[i][1]\n",
    "            new_spans.append((start_val, end_val))\n",
    "            i += 1\n",
    "\n",
    "        # Add the updated list of sentence spans to the article dictionary.\n",
    "        article['sentences'] = new_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sentence spans in set train_platinum...\n"
     ]
    }
   ],
   "source": [
    "get_sentence_spans(train_platinum, \"train_platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if sentence spans have been computed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_spans(data: dict, data_name: str):\n",
    "    print(f\"Checking sentence spans in set {data_name}...\")\n",
    "\n",
    "    for pmid, article in data.items():\n",
    "        # Process each entity in the article.\n",
    "        for entity in article['entities']:\n",
    "            location = entity['location']\n",
    "            start = entity['start_idx']\n",
    "            end = entity['end_idx']\n",
    "            # For title entities, we do nothing regarding sentence spans.\n",
    "            if location == 'title':\n",
    "                continue\n",
    "\n",
    "            # Only process abstract entities.\n",
    "            if location == 'abstract':\n",
    "                start_sentence = None\n",
    "                end_sentence = None\n",
    "\n",
    "                # Iterate over the original sentence spans to determine in which sentences the entity start and end fall.\n",
    "                for idx, s in enumerate(article['sentences']):\n",
    "                    # Using >= and <= to include boundaries.\n",
    "                    if start >= s[0] and start <= s[1] and start_sentence is None:\n",
    "                        start_sentence = idx\n",
    "                        #print(f'Start sentence assigned: {idx}')\n",
    "                    if end >= s[0] and end <= s[1] and end_sentence is None:\n",
    "                        end_sentence = idx\n",
    "                        #print(f'End sentence assigned: {idx}')\n",
    "\n",
    "                if start_sentence is None:\n",
    "                    raise Exception(f'{pmid} - Start sentence not assigned for entity: {entity}')\n",
    "                if end_sentence is None:\n",
    "                    raise Exception(f'{pmid} - End sentence not assigned for entity: {entity}')\n",
    "                \n",
    "                # If the entity falls in two different sentences, raise Exception.\n",
    "                if start_sentence != end_sentence:\n",
    "                      raise Exception(f'{pmid} - Entity assigned to two different sentences ({start_sentence}, {end_sentence}): {entity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking sentence spans in set train_platinum...\n"
     ]
    }
   ],
   "source": [
    "check_sentence_spans(train_platinum, \"train_platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map tokens to the sentence in which they are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens_to_sentences(data: dict, data_name: str):\n",
    "    \"\"\"\n",
    "    For each article, map tokens in the 'tokenized abstract' to the sentence in which they are located.\n",
    "    Uses the 'sentences' field in the article, which is assumed to be a list of (start, end) tuples.\n",
    "    \n",
    "    The mapping is stored as a dictionary where the key is the token index (its position in the tokenized abstract)\n",
    "    and the value is the sentence index. For example, if the first token belongs to sentence 0 and the third token\n",
    "    belongs to sentence 1, the mapping will include entries {0: 0, 2: 1}.\n",
    "    \n",
    "    Raises an Exception if a token does not fall within any of the sentence spans.\n",
    "    \"\"\"\n",
    "    print(f\"Mapping tokens to sentences in set {data_name}...\")\n",
    "\n",
    "    for pmid, article in data.items():\n",
    "        # Retrieve the tokenized abstract and the sentence spans.\n",
    "        tokens = article.get('tokenized_abstract')\n",
    "        sentences = article.get('sentences')\n",
    "        \n",
    "        if tokens is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'tokenized abstract'.\")\n",
    "        if sentences is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'sentences'. Make sure to run get_sentence_spans first.\")\n",
    "        \n",
    "        token_to_sentence = {}\n",
    "        \n",
    "        # Iterate over each token and determine which sentence it belongs to.\n",
    "        for token_index, token_entry in enumerate(tokens):\n",
    "            # Each token_entry is assumed to be a tuple: (token_text, start_offset, end_offset)\n",
    "            token_text, token_start, token_end = token_entry\n",
    "            assigned_sentence = None\n",
    "            \n",
    "            # Check each sentence span to see if the token falls within it.\n",
    "            for sentence_index, (sent_start, sent_end) in enumerate(sentences):\n",
    "                # We assume a token belongs to a sentence if its start is >= sentence start and its end is <= sentence end.\n",
    "                if token_start >= sent_start and token_end <= sent_end:\n",
    "                    assigned_sentence = sentence_index\n",
    "                    break  # Stop once we find the sentence that contains the token.\n",
    "            \n",
    "            if assigned_sentence is None:\n",
    "                raise Exception(\n",
    "                    f\"Token '{token_text}' (index {token_index}, offsets {token_start}-{token_end}) \"\n",
    "                    f\"in article {pmid} does not fall within any sentence span: {sentences}\"\n",
    "                )\n",
    "            \n",
    "            token_to_sentence[token_index] = assigned_sentence\n",
    "        \n",
    "        # Add the mapping to the article dictionary.\n",
    "        article['tokens_to_sentences_map'] = token_to_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping tokens to sentences in set train_platinum...\n"
     ]
    }
   ],
   "source": [
    "map_tokens_to_sentences(train_platinum, \"train_platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map entities to the token positions within each sentence containing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entities_to_tokens_within_sentences(data: dict, data_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    For each article, this function maps each entity (assumed to be in the abstract)\n",
    "    to the token positions within the sentence that contains it.\n",
    "    \n",
    "    For each entity in article['entities'] (with location 'abstract'), it adds:\n",
    "      - 'located_in_sentence': the sentence index in which the entity's tokens are located,\n",
    "      - 'start_token_in_sentence': the position of the entity's start token within that sentence,\n",
    "      - 'end_token_in_sentence': the position of the entity's end token within that sentence.\n",
    "    \n",
    "    This function relies on:\n",
    "      - article['tokenized abstract']: a list of tokens of the form (token_text, start_offset, end_offset)\n",
    "      - article['tokens_to_sentences_map']: a mapping { token_index -> sentence_index }\n",
    "      - article['sentences']: a list of (start, end) sentence spans for the abstract.\n",
    "    \"\"\"\n",
    "    print(f\"Mapping entities to tokens within sentences in set {data_name}...\")\n",
    "    \n",
    "    for pmid, article in data.items():\n",
    "        # Retrieve required fields.\n",
    "        tokens = article.get('tokenized_abstract')\n",
    "        token_to_sentence = article.get('tokens_to_sentences_map')\n",
    "        sentences = article.get('sentences')\n",
    "        \n",
    "        if tokens is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'tokenized abstract'.\")\n",
    "        if token_to_sentence is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'tokens_to_sentences_map'. Run map_tokens_to_sentences first.\")\n",
    "        if sentences is None:\n",
    "            raise Exception(f\"Article {pmid} is missing 'sentences'. Run get_sentence_spans first.\")\n",
    "        \n",
    "        # Build a helper mapping: for each sentence index, list the token indices that fall into that sentence.\n",
    "        sentence_to_token_indices = {}\n",
    "        for token_index in range(len(tokens)):\n",
    "            sent_idx = token_to_sentence.get(token_index)\n",
    "            if sent_idx is None:\n",
    "                raise Exception(\n",
    "                    f\"In article {pmid}, token index {token_index} is not mapped to any sentence. Tokens: {tokens[token_index]}\"\n",
    "                )\n",
    "            sentence_to_token_indices.setdefault(sent_idx, []).append(token_index)\n",
    "        \n",
    "        # Now process each entity.\n",
    "        for entity in article.get('entities', []):\n",
    "            if entity.get('location') != 'abstract': # Only process entities in the abstract.\n",
    "                continue\n",
    "            \n",
    "            # Retrieve the token indices for this entity.\n",
    "            entity_start_token = entity.get('start_token')\n",
    "            entity_end_token = entity.get('end_token')\n",
    "            \n",
    "            if entity_start_token is None or entity_end_token is None:\n",
    "                raise Exception(\n",
    "                    f\"Entity in article {pmid} is missing start_token or end_token: {entity}\"\n",
    "                )\n",
    "            \n",
    "            # Determine the sentence in which the entity's tokens are located.\n",
    "            sentence_for_start = token_to_sentence.get(entity_start_token)\n",
    "            sentence_for_end = token_to_sentence.get(entity_end_token)\n",
    "            \n",
    "            if sentence_for_start is None or sentence_for_end is None:\n",
    "                raise Exception(\n",
    "                    f\"Entity in article {pmid} has tokens not mapped to any sentence: {entity}\"\n",
    "                )\n",
    "            \n",
    "            if sentence_for_start != sentence_for_end:\n",
    "                raise Exception(\n",
    "                    f\"Entity in article {pmid} spans multiple sentences (start in {sentence_for_start}, end in {sentence_for_end}): {entity}\"\n",
    "                )\n",
    "            \n",
    "            located_sentence = sentence_for_start  # or sentence_for_end, both are same.\n",
    "            \n",
    "            # Get the list of token indices for the sentence.\n",
    "            tokens_in_sentence = sentence_to_token_indices.get(located_sentence)\n",
    "            if tokens_in_sentence is None:\n",
    "                raise Exception(\n",
    "                    f\"Sentence {located_sentence} not found in helper mapping for article {pmid}.\"\n",
    "                )\n",
    "            \n",
    "            # Find the position within the sentence for the start token.\n",
    "            try:\n",
    "                start_token_in_sentence = tokens_in_sentence.index(entity_start_token)\n",
    "            except ValueError:\n",
    "                raise Exception(\n",
    "                    f\"Entity start token {entity_start_token} not found in sentence tokens {tokens_in_sentence} for article {pmid}.\"\n",
    "                )\n",
    "            \n",
    "            # And the position within the sentence for the end token.\n",
    "            try:\n",
    "                end_token_in_sentence = tokens_in_sentence.index(entity_end_token)\n",
    "            except ValueError:\n",
    "                raise Exception(\n",
    "                    f\"Entity end token {entity_end_token} not found in sentence tokens {tokens_in_sentence} for article {pmid}.\"\n",
    "                )\n",
    "            \n",
    "            # Add the new fields to the entity.\n",
    "            entity['located_in_sentence'] = located_sentence\n",
    "            entity['start_token_in_sentence'] = start_token_in_sentence\n",
    "            entity['end_token_in_sentence'] = end_token_in_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping entities to tokens within sentences in set train_platinum...\n"
     ]
    }
   ],
   "source": [
    "map_entities_to_tokens_within_sentences(train_platinum, \"train_platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map each relation's subject and object to an index corresponding to their position in the article's 'entities' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_relations_to_entities(data: dict, data_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    For each article, this function maps each relation's subject and object to an index \n",
    "    corresponding to their position in the article's 'entities' list. It adds two new fields \n",
    "    to each relation:\n",
    "      - 'subject_entity_idx': the index of the subject entity in the article's 'entities' list.\n",
    "      - 'object_entity_idx': the index of the object entity in the article's 'entities' list.\n",
    "      \n",
    "    The matching is performed based on:\n",
    "      - For the subject:\n",
    "            relation['subject_location'] == entity['location']\n",
    "            relation['subject_start_idx'] == entity['start_idx']\n",
    "            relation['subject_end_idx'] == entity['end_idx']\n",
    "      - For the object:\n",
    "            relation['object_location'] == entity['location']\n",
    "            relation['object_start_idx'] == entity['start_idx']\n",
    "            relation['object_end_idx'] == entity['end_idx']\n",
    "            \n",
    "    It is assumed that the article['entities'] list is ordered such that title entities come first (starting at index 0)\n",
    "    and abstract entities follow (with indices continuing from the last title entity index).\n",
    "    \"\"\"\n",
    "    print(f\"Mapping relations to entities in set {data_name}...\")\n",
    "    \n",
    "    for pmid, article in data.items():\n",
    "        entities = article.get('entities')\n",
    "        relations = article.get('relations')\n",
    "        \n",
    "        if entities is None:\n",
    "            raise Exception(f\"Article {pmid} is missing the 'entities' field.\")\n",
    "        if relations is None:\n",
    "            # If there are no relations, there's nothing to map.\n",
    "            continue\n",
    "        \n",
    "        # Process each relation.\n",
    "        for relation in relations:\n",
    "            # Map the subject.\n",
    "            subject_idx_found = None\n",
    "            for idx, entity in enumerate(entities):\n",
    "                if (entity.get('location') == relation.get('subject_location') and\n",
    "                    entity.get('start_idx') == relation.get('subject_start_idx') and\n",
    "                    entity.get('end_idx') == relation.get('subject_end_idx')):\n",
    "                    subject_idx_found = idx\n",
    "                    break\n",
    "                    \n",
    "            if subject_idx_found is None:\n",
    "                raise Exception(\n",
    "                    f\"Subject entity not found for relation in article {pmid}: {relation}\"\n",
    "                )\n",
    "            relation['subject_entity_idx'] = subject_idx_found\n",
    "            \n",
    "            # Map the object.\n",
    "            object_idx_found = None\n",
    "            for idx, entity in enumerate(entities):\n",
    "                if (entity.get('location') == relation.get('object_location') and\n",
    "                    entity.get('start_idx') == relation.get('object_start_idx') and\n",
    "                    entity.get('end_idx') == relation.get('object_end_idx')):\n",
    "                    object_idx_found = idx\n",
    "                    break\n",
    "                    \n",
    "            if object_idx_found is None:\n",
    "                raise Exception(\n",
    "                    f\"Object entity not found for relation in article {pmid}: {relation}\"\n",
    "                )\n",
    "            relation['object_entity_idx'] = object_idx_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping relations to entities in set train_platinum...\n"
     ]
    }
   ],
   "source": [
    "map_relations_to_entities(train_platinum, \"train_platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert processed annotations to the DocRED format used by ATLOP for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_docred_format(data: dict, data_name: str, is_test=False) -> list:\n",
    "    \"\"\"\n",
    "    Converts articles (in our intermediate format) to the DocRED format.\n",
    "    \n",
    "    For each article, a new dictionary is produced with the following keys:\n",
    "      - \"vertexSet\": a list of entity mentions (each entity becomes a list with one mention).\n",
    "          Each mention is a dict with:\n",
    "              \"pos\": [start_token_in_sentence, end_token_in_sentence],\n",
    "              \"type\": entity label,\n",
    "              \"sent_id\": sentence id (0 for title; abstract sentences are numbered starting at 1),\n",
    "              \"name\": the entity text span.\n",
    "      - \"labels\": a list of relations, each a dict with:\n",
    "              \"r\": relation predicate/label,\n",
    "              \"h\": subject_entity_idx,\n",
    "              \"t\": object_entity_idx,\n",
    "              \"evidence\": a list of two sentence ids: [subject sentence id, object sentence id] \n",
    "                          (or one element if both are the same).\n",
    "      - \"paper_id\": the id of the paper.\n",
    "      - \"title\": the title string of the article.\n",
    "      - \"sents\": a list of lists of tokens. The first entry is the tokenization of the title and subsequent\n",
    "                 entries are the tokenizations of the abstract sentences.\n",
    "    \n",
    "    For abstract sentence tokenization, we use the sentence spans in article['sentences'] (a list of (start, end) offsets)\n",
    "    and the tokenized abstract (article['tokenized abstract'], where each token is a tuple (token_text, start, end)).\n",
    "    \n",
    "    Returns a list of DocRED-formatted document dictionaries.\n",
    "    \"\"\"\n",
    "    print(f\"Converting articles to DocRED format for set {data_name}...\")\n",
    "\n",
    "    docred_docs = []\n",
    "    for pmid, article in data.items():\n",
    "        # 1. Build the vertexSet.\n",
    "        vertexSet = []\n",
    "        for entity in article.get('entities', []):\n",
    "            # Determine sentence id: title entities are sentence 0,\n",
    "            # and abstract entities use a pre-computed 'located_in_sentence' (plus 1).\n",
    "            if entity.get('location') == 'title':\n",
    "                sent_id = 0\n",
    "            else:\n",
    "                sent_id = entity.get('located_in_sentence', 0) + 1\n",
    "\n",
    "            # Determine token offsets within the sentence.\n",
    "            if entity.get('location') == 'title':\n",
    "                pos = [entity.get('start_token'), entity.get('end_token') + 1]\n",
    "            else:\n",
    "                pos = [entity.get('start_token_in_sentence'), entity.get('end_token_in_sentence') + 1]\n",
    "                \n",
    "            mention = {\n",
    "                \"pos\": pos,\n",
    "                \"type\": entity.get(\"label\").upper(),\n",
    "                \"sent_id\": sent_id,\n",
    "                \"name\": entity.get(\"text_span\")\n",
    "            }\n",
    "            # Each vertexSet entry is a list of one mention.\n",
    "            vertexSet.append([mention])\n",
    "        \n",
    "        # 2. Build the labels (relations).\n",
    "        labels = []\n",
    "        for relation in article.get(\"relations\", []):\n",
    "            subj_idx = relation.get(\"subject_entity_idx\")\n",
    "            obj_idx = relation.get(\"object_entity_idx\")\n",
    "            if subj_idx is None or obj_idx is None:\n",
    "                raise Exception(f\"Missing entity mapping in relation: {relation} in article {pmid}\")\n",
    "            \n",
    "            subj_entity = article[\"entities\"][subj_idx]\n",
    "            obj_entity = article[\"entities\"][obj_idx]\n",
    "            \n",
    "            if subj_entity.get(\"location\") == \"title\":\n",
    "                subj_sent = 0\n",
    "            else:\n",
    "                subj_sent = subj_entity.get(\"located_in_sentence\", 0) + 1\n",
    "                \n",
    "            if obj_entity.get(\"location\") == \"title\":\n",
    "                obj_sent = 0\n",
    "            else:\n",
    "                obj_sent = obj_entity.get(\"located_in_sentence\", 0) + 1\n",
    "            \n",
    "            relation_dict = {\n",
    "                \"r\": relation.get(\"predicate\").upper(),\n",
    "                \"h\": subj_idx,\n",
    "                \"t\": obj_idx,\n",
    "                \"evidence\": [subj_sent, obj_sent] if subj_sent != obj_sent else [subj_sent]\n",
    "            }\n",
    "            labels.append(relation_dict)\n",
    "        \n",
    "        # 3. Build the sents field.\n",
    "        # The first sentence is the tokenized title.\n",
    "        title_tokens = article.get(\"tokenized_title\", [])\n",
    "        tokens_in_title = [token[0] for token in title_tokens]\n",
    "        sents = [tokens_in_title]\n",
    "            \n",
    "        # For abstract sentences, use 'sentences' (spans) and 'tokenized_abstract'.\n",
    "        abstract_tokens = article.get(\"tokenized_abstract\", [])\n",
    "        abstract_sents = []\n",
    "        for span in article.get(\"sentences\", []):\n",
    "            s_start, s_end = span\n",
    "            tokens_in_sentence = []\n",
    "            for token, t_start, t_end in abstract_tokens:\n",
    "                if t_start >= s_start and t_end <= s_end:\n",
    "                    tokens_in_sentence.append(token)\n",
    "            abstract_sents.append(tokens_in_sentence)\n",
    "        sents.extend(abstract_sents)\n",
    "        \n",
    "        # 4. Get the title string.\n",
    "        doc_title = article[\"metadata\"][\"title\"]\n",
    "        print(pmid)\n",
    "        \n",
    "        # 5. Build the final document dictionary.\n",
    "        doc = {\n",
    "            \"vertexSet\": vertexSet,\n",
    "            \"labels\": labels,\n",
    "            \"paper_id\": pmid,\n",
    "            \"title\": doc_title,\n",
    "            \"sents\": sents\n",
    "        }\n",
    "        docred_docs.append(doc)\n",
    "    \n",
    "    return docred_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting articles to DocRED format for set train_platinum...\n",
      "38068763\n",
      "35965349\n",
      "34870091\n",
      "28158162\n",
      "34172092\n",
      "37845499\n",
      "37371676\n",
      "37574818\n",
      "37571393\n",
      "37841274\n",
      "37485660\n",
      "31955786\n",
      "34098340\n",
      "38350463\n",
      "29352709\n",
      "33511258\n",
      "33422110\n",
      "34985325\n",
      "36550591\n",
      "30459574\n",
      "38026003\n",
      "33194817\n",
      "29022384\n",
      "29857583\n",
      "34758889\n",
      "37881577\n",
      "36984505\n",
      "32979562\n",
      "34961418\n",
      "25034760\n",
      "33067915\n",
      "33271210\n",
      "36794003\n",
      "38132705\n",
      "36900437\n",
      "34603341\n",
      "34422393\n",
      "35914559\n",
      "38422755\n",
      "37228957\n",
      "30717162\n",
      "31248424\n",
      "37469436\n",
      "31179435\n",
      "37995075\n",
      "35326429\n",
      "31083360\n",
      "38010793\n",
      "31685046\n",
      "34444820\n",
      "34092293\n",
      "37927130\n",
      "35432226\n",
      "36757367\n",
      "36493975\n",
      "37213508\n",
      "33046051\n",
      "38204948\n",
      "31952911\n",
      "29023380\n",
      "28572752\n",
      "36346385\n",
      "32459708\n",
      "33177907\n",
      "38089822\n",
      "31646148\n",
      "23981537\n",
      "37657622\n",
      "36760344\n",
      "33722869\n",
      "34776854\n",
      "28976454\n",
      "31053995\n",
      "38576868\n",
      "37511699\n",
      "37464164\n",
      "37368331\n",
      "37396336\n",
      "36517709\n",
      "37978477\n",
      "33713734\n",
      "34830610\n",
      "29843470\n",
      "32469834\n",
      "37960197\n",
      "30707176\n",
      "38139446\n",
      "37209162\n",
      "36738999\n",
      "37627638\n",
      "30394313\n",
      "34393849\n",
      "37606895\n",
      "38508549\n",
      "36564391\n",
      "37095530\n",
      "35083314\n",
      "37377497\n",
      "37207228\n",
      "38707924\n",
      "30584306\n",
      "34422879\n",
      "26927355\n",
      "38409102\n",
      "38318337\n",
      "33105830\n",
      "33212907\n",
      "36698474\n",
      "37481569\n",
      "30818031\n",
      "37168424\n"
     ]
    }
   ],
   "source": [
    "docred_train_platinum = convert_to_docred_format(train_platinum, \"train_platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the dictionary variable to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_to_json(docred_dict, output_file_path):\n",
    "\tdict_with_double_quotes = json.dumps(docred_dict, ensure_ascii=False)\n",
    "\twith open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "\t\tf.write(dict_with_double_quotes)\n",
    "\n",
    "dump_to_json(docred_train_platinum, PATH_OUTPUT_PLATINUM_TRAIN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
